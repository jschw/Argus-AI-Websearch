{
    "llm_settings": [
        {
            "name": "Openai_Inference_Runner",
            "protocol": "tcp",
            "ip": "127.0.0.1",
            "port": "5514",
            "port_logger": "6514",
            "enable_dryrun": "1",
            "debug_output_cli": "1",
            "api_key": "<apikey>",
            "model_name": "gpt-3.5-turbo",
            "max_tokens": "450",
            "temperature": "1.0",
            "top_p": "0.5",
            "frequency_penalty": "0.5",
            "vectorstore_path": "../document_store/QDrant_vectorstore_demo",
            "document_context_size": "3",
            "add_full_context_always": "0"
        }
    ],

    "conversation_settings": [
        {
            "stage_1_depth": 3,
            "stage_2_depth": 2,
            "stage_3_chunksize": 700,
            "stage_3_depth": 3,
            "enable_debug_output_cli": "1",
            "dump_save_path": "saved_prompts/",
            "add_full_context_always": "0"
        }
    ],

    "upstream_nodes": [
        {
            "name": "Main_Task",
            "ip": "127.0.0.1",
            "port_subscriber": "5500",
            "subscriber_topic1": "comm_openai_inf_5514"
        }
    ]
}